{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10701d47",
   "metadata": {},
   "source": [
    "# Intro\n",
    "## General\n",
    "Machine learning allows the user to feed a computer algorithm an immense amount of data and have the computer analyze and make data-driven recommendations and decisions based on only the input data. \n",
    "In most of the situations we want to have a machine learning system to make **predictions**, so we have several categories of machine learning tasks depending on the type of prediction needed: **Classification, Regression, Clustering, Generation**, etc.\n",
    "\n",
    "**Classification** is the task whose goal is the prediction of the label of the class to which the input belongs (e.g., Classification of images in two classes: cats and dogs).\n",
    "**Regression** is the task whose goal is the prediction of numerical value(s) related to the input (e.g., House rent prediction, Estimated time of arrival ).\n",
    "**Generation** is the task whose goal is the creation of something new related to the input (e.g., Text translation, Audio beat generation, Image denoising ). **Clustering** is the task of grouping a set of objects in such a way that objects in the same group (called a **cluster**) are more similar (in some sense) to each other than to those in other **clusters** (e.g., Clients clutering).\n",
    "\n",
    "In machine learning, there are learning paradigms that relate to one aspect of the dataset: **the presence of the label to be predicted**. **Supervised Learning** is the paradigm of learning that is applied when the dataset has the label variables to be predicted, known as ` y variables`. **Unsupervised Learning** is the paradigm of learning that is applied when the dataset has not the label variables to be predicted. **Self-supervised Learning** is the paradigm of learning that is applied when part of the X dataset is considere as the label to be predicted (e.g., the Dataset is made of texts and the model try to predict the next word of each sentence).\n",
    "\n",
    "## Notebook overview\n",
    "This notebook contains the step by step approach undertaken to build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores; a large Ecuadorian-based grocery retailer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626680e0",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc5eac",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Here is the section to install all the packages/libraries that will be needed to tackle the challlenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de582fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas\n",
    "# pip install numpy \n",
    "# pip install matplotlib\n",
    "# pip install seaborn \n",
    "# pip install forex_python\n",
    "# pip install babel \n",
    "# pip install seaborn\n",
    "# pip install pandas-profiling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26799fdf",
   "metadata": {},
   "source": [
    "## Importation\n",
    "Here is the section to import all the packages/libraries that will be used through this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf2e97c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "from forex_python.converter import CurrencyRates\n",
    "from babel.numbers import format_currency\n",
    "import datetime as dt\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, trim_mean, mstats, mode\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import skew\n",
    "\n",
    "\n",
    "# Vizualisation (Matplotlib, Plotly, Seaborn, etc. )\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "from plotly.offline import plot\n",
    "from IPython.display import display\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import sweetviz as sv\n",
    "\n",
    "# balance data\n",
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Machine learning libraries and metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Feature Processing (Scikit-learn processing, etc. )\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, LabelEncoder, Binarizer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score,roc_curve, auc,roc_auc_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "# Other packages\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import patoolib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f436e09",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Here is the section to load the datasets (train, eval, test) and the additional files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb60dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidaysurl=\"https://raw.githubusercontent.com/Gilbert-B/Regression-Project/main/data/holidays_events.csv\"\n",
    "oilurl=\"https://raw.githubusercontent.com/Gilbert-B/Regression-Project/main/data/oil.csv\"\n",
    "storesurl=\"https://raw.githubusercontent.com/Gilbert-B/Regression-Project/main/data/stores.csv\"\n",
    "testurl=\"https://raw.githubusercontent.com/Gilbert-B/Regression-Project/main/data/test.csv\"\n",
    "trainurl=\"https://github.com/Gilbert-B/Regression-Project/releases/download/data/train.csv\"\n",
    "transactionsurl=\"https://raw.githubusercontent.com/Gilbert-B/Regression-Project/main/data/transactions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a4c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(testurl,error_bad_lines=False)\n",
    "Train_df= pd.read_csv(trainurl,error_bad_lines=False)\n",
    "Holiday_df= pd.read_csv(holidaysurl,error_bad_lines=False)\n",
    "Transaction_df= pd.read_csv(transactionsurl,error_bad_lines=False)\n",
    "stores_df= pd.read_csv(storesurl,error_bad_lines=False)\n",
    "oil_df= pd.read_csv(oilurl,error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e9ca05",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: EDA\n",
    "\n",
    "Details of the data cleaing process has been provided in the EDA notebook which can be found here:\n",
    "https://github.com/Gilbert-B/Regression-Project/blob/main/Notebook/Regression-Project_EDA.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895dc16c",
   "metadata": {},
   "source": [
    "# Feature Processing & Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9afe34",
   "metadata": {},
   "source": [
    "## Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1153aa63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find any duplicates in the columns \n",
    "Train_df.duplicated().any(), \n",
    "stores_df.duplicated().any(), \n",
    "oil_df.duplicated().any(),  \n",
    "test_df.duplicated().any(),  \n",
    "Holiday_df.duplicated().any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197d7f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date           0\n",
       "dcoilwtico    43\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking missing values\n",
    "null_values=pd.isnull(oil_df).sum()\n",
    "null_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306adb2",
   "metadata": {},
   "source": [
    "Oil Data has 43 missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Impute Missing Values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
